<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-11-05T19:14:33-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Yash Chandak</title><subtitle>y[lastname]@stanford.edu</subtitle><author><name>Yash Chandak</name></author><entry><title type="html">Towards Safe Policy Improvement for Non-Stationary MDPs</title><link href="http://localhost:4000/blog/spin" rel="alternate" type="text/html" title="Towards Safe Policy Improvement for Non-Stationary MDPs" /><published>2020-12-24T00:00:00-08:00</published><updated>2020-12-24T00:00:00-08:00</updated><id>http://localhost:4000/blog/SPIN</id><content type="html" xml:base="http://localhost:4000/blog/spin">&lt;div class=&quot;message&quot;&gt;
 This post is based on a NeurIPS 2020 paper that I worked on with &lt;a href=&quot;&quot;&gt; Scott Jordan&lt;/a&gt;, &lt;a href=&quot;https://research.adobe.com/person/georgios-theocharous/&quot;&gt; Georgios Theocharous&lt;/a&gt;, &lt;a href=&quot;https://webdocs.cs.ualberta.ca/~whitem/&quot;&gt;Martha White&lt;/a&gt;, and &lt;a href=&quot;https://people.cs.umass.edu/~pthomas/&quot;&gt;Philip Thomas&lt;/a&gt;. Here are the links for the &lt;a href=&quot;https://arxiv.org/abs/2010.12645&quot;&gt; paper&lt;/a&gt;, &lt;a href=&quot;https://nips.cc/virtual/2020/public/poster_680390c55bbd9ce416d1d69a9ab4760d.html&quot;&gt; video&lt;/a&gt;, and the &lt;a href=&quot;https://github.com/ScottJordan/SafePolicyImprovementNonstationary&quot;&gt;code&lt;/a&gt;.
&lt;/div&gt;

&lt;p&gt;Reinforcement learning (RL) methods have shown promise to be applicable to real-world sequential decision-making problems such as medical treatment, budget constrained bidding, and other high-risk high-reward applications. However, to actually deploying RL algorithms  for such real-world applications, safety guarantees are critical to mitigate serious risks in terms of both human-life and monetary assets. More concretely, here, by &lt;em&gt;safety&lt;/em&gt; we mean that any update to a system should not reduce the performance of an existing system (e.g., a doctor’s initially prescribed treatment). A further complication is that these applications are non-stationary, which violates the foundational assumption of &lt;em&gt;stationarity&lt;/em&gt; in most RL algorithms. This raises the main question we aim to address&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;How can we build sequential decision-making systems that provide safety guarantees for problems with non-stationarities?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this work, we merge ideas from time-series literature and model-free RL to&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Formalize the safe policy improvement problem for non-stationary MDPs, and provide an algorithm for addressing it.&lt;/li&gt;
  &lt;li&gt;Provide a user-controllable knob to set the desired &lt;em&gt;confidence level&lt;/em&gt;: the maximum admissible probability that a worse policy will be deployed.&lt;/li&gt;
  &lt;li&gt;Develop a method does not require building a model of a non-stationary MDP.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, our method is designed for environments where changes are smooth and gradual, and might not be ideal for environments where the changes are abrupt.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;/h2&gt;

&lt;p&gt;In a non-stationary setting, the MDP that an agent might face after deployment might be different than the ones it has encountered before. Therefore, in a non-stationary setting we need to ensure safety with respect to the future MDPs that an agent might face.  To formalize this, let &lt;script type=&quot;math/tex&quot;&gt;\mathcal D&lt;/script&gt; be any available historical data and &lt;script type=&quot;math/tex&quot;&gt;\texttt{alg}(\mathcal D)&lt;/script&gt; be a policy returned by an algorithm after analyzing the data.  Let &lt;script type=&quot;math/tex&quot;&gt;\pi^\text{safe}&lt;/script&gt; be a known safe policy and &lt;script type=&quot;math/tex&quot;&gt;(1-\alpha) \in [0,1]&lt;/script&gt; be the &lt;em&gt;safety level&lt;/em&gt; desired by the user. Let &lt;script type=&quot;math/tex&quot;&gt;\rho(\pi, i)&lt;/script&gt; be the performance of a policy &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; in episode &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; and let &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; be the last observed episode number such that episode numbers greater than &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; belong to the future. Then we aim to ensure, with high-confidence, that the future performance of the updated policy will provide improvement over the existing &lt;script type=&quot;math/tex&quot;&gt;\pi^\text{safe}&lt;/script&gt;. That is,&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
    \Pr \Big(\rho(\texttt{alg}(\mathcal D), {k+\delta}) \geq \rho(\pi^\text{safe}, k + \delta) \Big) \geq 1 - \alpha. 
\end{align}&lt;/script&gt;

&lt;p&gt;While it is desirable to ensure the safety guarantee of the form mentioned above, obtaining a new policy from &lt;script type=&quot;math/tex&quot;&gt;\texttt{alg}(\mathcal D)&lt;/script&gt; that meets this requirement might not be possible unless some more regularity assumptions are imposed on the problem.  For example, if the environment can change arbitrarily, then there is not much hope of estimating the future performance to ensure safety.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
One way to make the problem more tractable could be to assume Lipschitz smoothness on the dynamics and rewards. However, we show a negative result in the paper and discuss why Lipschitz smoothness might be insufficient for our purpose. Therefore, to make the problem more tractable, we make an alternate assumption that the performance of any policy changes &lt;em&gt;smoothly&lt;/em&gt; over time. That is, let &lt;script type=&quot;math/tex&quot;&gt;M_i&lt;/script&gt; be the MDP in episode &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; and let &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; be a non-linear basis function
(like Fourier basis, which is popular for modeling time-series trends) then&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Assume: For every policy &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;, there exists a sequence of mean-zero and independent noises &lt;script type=&quot;math/tex&quot;&gt;\{\xi_i\}_{i=1}^{k+\delta}&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;\exists w \in \mathbb{R}^{d \times 1}&lt;/script&gt;, such that, &lt;script type=&quot;math/tex&quot;&gt;\forall i \in [1, k+\delta], \,\,\,  \rho(\pi, M_i) = \phi(i)w + \xi_i&lt;/script&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;time-series-forecasting-and-sequential-hypothesis-testing&quot;&gt;Time-series forecasting and sequential hypothesis testing&lt;/h2&gt;

&lt;p&gt;To address this problem, we build upon our  &lt;a href=&quot;https://icml.cc/virtual/2020/poster/6316&quot;&gt;previous work&lt;/a&gt; on non-stationary MDPs, which combines counterfactual reasoning with curve-fitting to estimate the future performance of a policy. That is, for any policy we first estimate its performances on all the past episodes using off-policy evaluation. This &lt;em&gt;univariate&lt;/em&gt; sequence of performance estimates are then analyzed to make a performance forecast based on the trend resulting due to the underlying non-stationarity (see here for a short &lt;a href=&quot;/blog/prognosticator&quot;&gt;blogpost&lt;/a&gt; on that work).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
 &lt;img src=&quot;/images/blog/spin/SafeNSRLidea.png&quot; alt=&quot;idea&quot; style=&quot;width: 65%; border-radius:0%&quot; /&gt;
 &lt;/p&gt;

&lt;p&gt;The figure above summarizes the main idea, where &lt;script type=&quot;math/tex&quot;&gt;\pi_c&lt;/script&gt; is a candidate policy which could potentially be used to provide improvement over &lt;script type=&quot;math/tex&quot;&gt;\pi^\text{safe}&lt;/script&gt;. The gray dots correspond to the returns, &lt;script type=&quot;math/tex&quot;&gt;G(\beta)&lt;/script&gt;, observed for any behavior policy &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; that was used to collect the data. The red and the blue dots correspond to the counterfactual estimates, &lt;script type=&quot;math/tex&quot;&gt;\hat \rho(\pi_c)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\hat \rho(\pi^\text{safe})&lt;/script&gt;, for performances of &lt;script type=&quot;math/tex&quot;&gt;\pi_c&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\pi^\text{safe}&lt;/script&gt;, respectively, in different episodes. The shaded regions correspond to the uncertainty in future performance. If we could estimate this uncertainty by analysing the trend of the counterfactual estimates for the past performances, then we could check whether the lower bound on &lt;script type=&quot;math/tex&quot;&gt;\pi_c&lt;/script&gt;’s future performance is higher than the upper bound on &lt;script type=&quot;math/tex&quot;&gt;\pi^\text{safe}&lt;/script&gt;’s future performance or not? If it is indeed higher, then we can say with high-confidence that &lt;script type=&quot;math/tex&quot;&gt;\pi_c&lt;/script&gt; will be safe, otherwise we can say that we do not have enough confidence to provide a safe update and therefore just re-execute  &lt;script type=&quot;math/tex&quot;&gt;\pi^\text{safe}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
     &lt;img src=&quot;/images/blog/spin/SafeNSRL.png&quot; alt=&quot;seldonian&quot; style=&quot;vertical-align:middle; width: 65%; margin:0px 10px; border-radius:0%&quot; /&gt;
     &lt;/p&gt;

&lt;p&gt;Given a procedure to obtain uncertainty in a policy’s future performance, the above diagram depicts the steps of the proposed algorithm to ensure safety in the presence of non-stationarity. These steps build upon the template for &lt;a href=&quot;https://aisafety.cs.umass.edu/&quot;&gt;Seldonian algorithms&lt;/a&gt; and perform sequential hypothesis (A/B) testing. That is, the proposed algorithm first partitions the initial data &lt;script type=&quot;math/tex&quot;&gt;\mathcal D_1&lt;/script&gt; into two sets, namely &lt;script type=&quot;math/tex&quot;&gt;\mathcal D_\text{train}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathcal D_\text{test}&lt;/script&gt;. Subsequently, &lt;script type=&quot;math/tex&quot;&gt;\mathcal D_\text{train}&lt;/script&gt; is used to search for a possible &lt;em&gt;candidate policy&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;\pi_c&lt;/script&gt; that might improve the future performance, and &lt;script type=&quot;math/tex&quot;&gt;\mathcal D_\text{test}&lt;/script&gt; is used to perform a safety test on the proposed candidate policy &lt;script type=&quot;math/tex&quot;&gt;\pi_c&lt;/script&gt;. The existing safe policy &lt;script type=&quot;math/tex&quot;&gt;\pi^\text{safe}&lt;/script&gt; is only updated if the proposed policy &lt;script type=&quot;math/tex&quot;&gt;\pi_c&lt;/script&gt; passes the safety test, and this process continues…&lt;/p&gt;

&lt;h2 id=&quot;uncertainty-estimation&quot;&gt;Uncertainty estimation&lt;/h2&gt;

&lt;p&gt;The above discussion of the main idea relies upon a method to quantify uncertainty in a policy’s future performance. Quantifying this uncertainty requires a careful consideration of the following three factors.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
     &lt;img src=&quot;/images/blog/spin/safeNSRL_challenges.png&quot; alt=&quot;challenges&quot; style=&quot;vertical-align:middle; width: 75%; margin:0px 10px; border-radius:0%&quot; /&gt;
     &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Single sequence of MDPs&lt;/em&gt;: Notice that we only get to observe a &lt;em&gt;single&lt;/em&gt; sequence of MDPs. Further, since the MDP can change after every episode due to the underlying non-stationarity, only a &lt;em&gt;single&lt;/em&gt; episode per MDP might be available.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Heavy-tailed distributions&lt;/em&gt;: Recall that the off-policy estimation of a policy’s performance requires the use of importance sampling that can lead to extremely heavy-tailed and asymmetric distributions.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Heteroskedasticity&lt;/em&gt;: Since the executed policy can be different for different MDPs in the past, the behavior policy &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; for importance sampling may be different for different episodes.  This results in heteroskedasticity, which is further exacerbated by the changes in the sources of stochasticity as the MDPs change over episodes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;wild-bootstrap&quot;&gt;Wild bootstrap&lt;/h4&gt;

&lt;p&gt;To quantify uncertainty in the future performance while accounting for the above factors, we leverage a technique known as &lt;code class=&quot;highlighter-rouge&quot;&gt;Wild bootstrap&lt;/code&gt; from the time-series literature. This allows construction of upto &lt;script type=&quot;math/tex&quot;&gt;2^k&lt;/script&gt; &lt;em&gt;similar&lt;/em&gt; sequence of past performances from a single sequence of the original &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; performance estimates, and also tackles heteroskedasticity and non-normal distributions! Having obtained this set of similar sequences of past performances, we first compute an &lt;em&gt;empirical distribution&lt;/em&gt; of future performances from it and then consequently obtain the required upper and lower bounds for the future performance. Further, under certain regularity conditions, we show in our paper that the bounds on the future performance obtained using wild-bootstrap are consistent. We outline the steps for this procedure using wild-bootstrap below:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Let  &lt;script type=&quot;math/tex&quot;&gt;Y^+ := [\rho(\pi, 1), ..., \ \rho(\pi, k)]^\top \in \mathbb{R}^{k \times 1}&lt;/script&gt; correspond to the true performances of &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;.  Create &lt;script type=&quot;math/tex&quot;&gt;\hat Y = \Phi(\Phi^\top  \Phi)^{-1}\Phi^\top Y&lt;/script&gt;, a least-squares estimate of &lt;script type=&quot;math/tex&quot;&gt;Y^+&lt;/script&gt;, using the counterfactual performance estimates &lt;script type=&quot;math/tex&quot;&gt;Y := [\hat \rho(\pi, 1), ..., \ \hat\rho(\pi, k)]^\top&lt;/script&gt; and obtain the regression errors  &lt;script type=&quot;math/tex&quot;&gt;\hat \xi := \hat Y - Y&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create pseudo-noises &lt;script type=&quot;math/tex&quot;&gt;\xi^* := \hat \xi \odot \sigma^*&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;\odot&lt;/script&gt; represents Hadamard product and &lt;script type=&quot;math/tex&quot;&gt;\sigma^* \in \mathbb{R}^{k \times 1}&lt;/script&gt; is Rademacher random variable (i.e., &lt;script type=&quot;math/tex&quot;&gt;\forall i \in [1,k], \,\, \Pr(\sigma_i^*=+1) = \Pr(\sigma_i^*=-1) = 0.5&lt;/script&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create pseudo-performances  &lt;script type=&quot;math/tex&quot;&gt;Y^* := \hat Y + \xi^*&lt;/script&gt;, to obtain pseudo-samples for &lt;script type=&quot;math/tex&quot;&gt;\hat Y&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\hat \rho (\pi, {k+\delta})&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;\hat Y^* = \Phi(\Phi^\top  \Phi)^{-1}\Phi^\top Y^*&lt;/script&gt; and  &lt;script type=&quot;math/tex&quot;&gt;\hat \rho (\pi, {k+\delta})^* = \phi(k+\delta)(\Phi^\top  \Phi)^{-1}\Phi^\top Y^*.&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
     &lt;img src=&quot;/images/blog/spin/diffLB.png&quot; alt=&quot;candidate&quot; style=&quot;vertical-align:middle; width: 70%; margin:0px 10px; border-radius:0%&quot; /&gt;
     &lt;/p&gt;

&lt;p&gt;The above image provides a computational graph for quantifying the uncertainty in the forecasted performance.  Wild bootstrap leverages Rademacher variables &lt;script type=&quot;math/tex&quot;&gt;\sigma^*&lt;/script&gt; and the errors from regression to repeat steps 2 and 3 above to create pseudo-performances. Based on these pseudo-performances, an empirical distribution and thus also the lower bound &lt;script type=&quot;math/tex&quot;&gt;\hat \rho^\text{lb}&lt;/script&gt; for the future performance is obtained. We also show in our paper how the derivative of this lower bound &lt;script type=&quot;math/tex&quot;&gt;\hat \rho^\text{lb}&lt;/script&gt; with respect to the policy parameters can be computed to search for a candidate policy &lt;script type=&quot;math/tex&quot;&gt;\pi_c&lt;/script&gt; that maximizes the lower bound. Such a candidate policy will then have the highest chances of passing the safety test and providing a safe policy update.&lt;/p&gt;

&lt;h2 id=&quot;empirical-performance&quot;&gt;Empirical performance&lt;/h2&gt;

&lt;p&gt;We provide an empirical analysis on two simulated domains inspired by safety-critical real-world problems that exhibit non-stationarity. One is for the treatment of Type-1 diabetes and the other is for a recommender system. To understand the impact of different rates of non-stationarity, we conduct the experiment for varying degrees of &lt;em&gt;speed&lt;/em&gt;, where speed of 0 represents stationary environment and higher speeds indicate quicker changes in the environment.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
 &lt;img src=&quot;/images/blog/spin/results.png&quot; alt=&quot;results&quot; style=&quot;vertical-align:middle; width: 60%; margin:0px 10px; border-radius:0%&quot; /&gt;
 &lt;/p&gt;

&lt;p&gt;The above figure illustrates the safety violation rates and performances for the proposed method, SPIN, and a baseline safe policy improvement method that does not account for non-stationarity. Top-left plot illustrates a typical learning curve, where it can be noticed that SPIN updates a policy whenever there is room for a significant improvement over the (unknown) performance of &lt;script type=&quot;math/tex&quot;&gt;\pi^\text{safe}&lt;/script&gt;. In the middle and the right plots it can be seen that both SPIN and the baseline method ensure safety in the stationary setting but the baseline method can result in unsafe policies more than the desired 5% threshold in the non-stationary setting. In comparison, SPIN provides policy improvement and maintains the failure rate near the desired 5% threshold always.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
 Further, our experimental results call into question the popular misconception that the stationarity assumption is not severe when changes are slow. In fact, it can be quite the opposite: Slow changes can be more deceptive and can make existing algorithms, which do not account for non-stationarity, more susceptible to deploying unsafe policies. For more details, we encourage the readers to check out the &lt;a href=&quot;https://arxiv.org/abs/2010.12645&quot;&gt; paper&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;open-questions&quot;&gt;Open Questions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Our method relied upon an assumption that the performance of a policy changes smoothly over time in a non-stationary environment. While this seems natural for many problem settings, it is certainly not true always. Thankfully, identifying jumps or the function class best suited to model the trend is well-studied in the time-series literature. Can we leverage their goodness-of-fit tests to find a good parameterization for the trend or warn the user when this assumption is not true?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Our smoothness assumption also departs from the conventional way of assuming Lipschitz smoothness on the dynamics and the reward function. However, this opens up the question about what does smoothness on the policy performance correspond to in the terms of changes in the dynamics and the reward function?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yash Chandak</name></author><category term="timeseries" /><category term="nonstationary" /><category term="reinforcement" /><category term="safety" /><category term="bootstrap" /><summary type="html">How can we ensure in a non-stationary MDP that proposed policy updates provide improvement over an existing policy, with high-confidence?</summary></entry><entry><title type="html">Optimizing for the Future in Non-Stationary MDPs</title><link href="http://localhost:4000/blog/prognosticator" rel="alternate" type="text/html" title="Optimizing for the Future in Non-Stationary MDPs" /><published>2020-12-23T00:00:00-08:00</published><updated>2020-12-23T00:00:00-08:00</updated><id>http://localhost:4000/blog/OptFuture</id><content type="html" xml:base="http://localhost:4000/blog/prognosticator">&lt;div class=&quot;message&quot;&gt;
  This post is based on an ICML 2020 paper that I worked on with &lt;a href=&quot;https://research.adobe.com/person/georgios-theocharous/&quot;&gt; Georgios Theocharous &lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/citations?user=yK56jugAAAAJ&amp;amp;hl=en&quot;&gt;Shiv Shankar&lt;/a&gt;, &lt;a href=&quot;https://webdocs.cs.ualberta.ca/~whitem/&quot;&gt;Martha White&lt;/a&gt;, &lt;a href=&quot;https://people.cs.umass.edu/~mahadeva/Site/About_Me.html&quot;&gt;Sridhar Mahadevan&lt;/a&gt;, and &lt;a href=&quot;https://people.cs.umass.edu/~pthomas/&quot;&gt;Philip Thomas&lt;/a&gt;. Here are the links for the &lt;a href=&quot;https://arxiv.org/abs/2005.08158&quot;&gt;paper&lt;/a&gt;, &lt;a href=&quot;https://icml.cc/virtual/2020/poster/6316&quot;&gt;video&lt;/a&gt;,  and the &lt;a href=&quot;https://github.com/yashchandak/OptFuture_NSMDP&quot;&gt; code&lt;/a&gt;.
&lt;/div&gt;

&lt;p&gt;Reinforcement learning algorithms typically assume that the reward function and the transition dynamics are fixed, that is the underlying MDP is &lt;em&gt;stationary&lt;/em&gt;. This assumption is so deeply rooted within the current methods that it is not even explicitly mentioned. However, this assumption is often violated in many practical problems of interest. For example, consider an automated medical care system. Over time, human physiology changes leading to a change in the impact of a medication on a patient. Similarly, consider a robotics system. Over time, motors gradually suffer from wear and tear, leading to increased friction. Further, in almost all human-computer interaction applications, e.g., dialogue systems, recommedner systems, online marketing, etc., human behavior gradually changes over time. In such scenarios, if the automated system is not adapted to take such changes into account then the system might quickly become sub-optimal.  This raises our main question:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;How do we build systems that &lt;em&gt;proactively&lt;/em&gt; search for a policy that will be good for the future MDP?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In this work, we merge ideas from model-free reinforcement learning and time-series literature to develop a method that&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Concisely models the effect of changes in the environment on a policy’s performance using a &lt;em&gt;univariate&lt;/em&gt; time-series.&lt;/li&gt;
  &lt;li&gt;Does not require modeling the transition and reward functions.&lt;/li&gt;
  &lt;li&gt;Can leverage all the past data and is data-efficient.&lt;/li&gt;
  &lt;li&gt;Mitigates performance lag by proactively optimizing performance for future episodes.&lt;/li&gt;
  &lt;li&gt;It degenerates to an estimator of the ordinary policy gradient if the system is stationary.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, our method is designed for environments where changes are smooth and gradual, and might not be ideal for environments where the changes are abrupt.&lt;/p&gt;

&lt;h2 id=&quot;a-time-series-perspective&quot;&gt;A time-series perspective&lt;/h2&gt;

&lt;p&gt;In the non-stationary setting, it would have been ideal if we had some samples from the future MDP so that we could search for a policy &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; beforehand, which would be good for the MDP the agent faces when deployed. However, getting even a single data sample from the future MDP is impossible. All that we can expect is some data collected using a behavior policy &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; for the MDPs the agent had interacted with in the past episodes. 
  Therefore, to estimate the future performance of a policy &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; we use an indirect procedure by first asking a counter-factual question:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;What would have been &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;’s performances, if we had executed &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; in the past episodes?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This can be easily answered using off-policy performance estimation. Let the subscripts denote the episode number and superscripts denote the timestep within an epsiode, then an unbiased estimate of the performance of &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; in the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;‘th episode is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \hat J_i(\pi) &amp;:=  \sum_{t=0}^{H} \left ( \prod_{l=0}^t 
    \frac{\pi(A_i^l|S_i^l)}{\beta_i(A_i^l|S_i^l)} \right) \gamma^t R_i^t. \label{eqn:ope}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Having obtained the counter-factual estimate of &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;’s performances in the past episodes, we now ask the other question,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;’s performance in the past episodes are known, can we forecast &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;’s future performance?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Indeed, since &lt;script type=&quot;math/tex&quot;&gt;\forall i, \,\, \hat J_i(\pi)&lt;/script&gt; is an unbiased estimates of &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;’s performance, these estimates can be analyzed to understand how &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;’s performance has been changing due to the underlying non-stationarity. Therefore, a simple curve-fitting can now be performed on the counter-factual performance estimates to reveal the performance trend over time. Using this analysis we can then forecast the future performance for &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
 &lt;img src=&quot;/images/blog/optfuture/idea2.png&quot; alt=&quot;eval&quot; style=&quot;width: 60%;  margin:20px 20px; border-radius:0%&quot; /&gt;
 &lt;/p&gt;

&lt;p&gt;The above images illustrate the key idea. In a stationary environment, since the performance of &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; is constant across episodes, off-policy evaluation provides a noisy estimate about a constant value for &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;’s past performances. In comparison, in a non-stationary setting, off-policy evaluation provides a noisy estimate about a time-varying value for &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;’s past performances. Therefore, irrespective of the domain complexity, &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;’s future performance &lt;script type=&quot;math/tex&quot;&gt;J_{k+1}(\theta)&lt;/script&gt; can now be forecasted using a &lt;em&gt;univariate&lt;/em&gt; time series function &lt;script type=&quot;math/tex&quot;&gt;\Psi&lt;/script&gt; that takes as input the counter-factual performance estimates for the past episodes,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
    \hat J_{k+1}(\theta) :=  \Psi (\hat J_1(\pi), \hat J_2(\pi), ...., \hat J_k(\pi)). \label{eqn:forecaster}
\end{align}&lt;/script&gt;

&lt;p&gt;Here is a simple example using least-squares regression. Let &lt;script type=&quot;math/tex&quot;&gt;X := [1, 2, ..., k]^\top \in \mathbb{R}^{k \times 1}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y := [\hat J_1(\pi), \hat J_2(\pi), \hat J_2(\pi), ..., \hat J_k(\pi)]^\top  \in \mathbb{R}^{k \times 1}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; is the number for the last episode observed. Since performance trends need not be linear, let &lt;script type=&quot;math/tex&quot;&gt;\phi(x) \in \mathbb{R}^{1 \times d}&lt;/script&gt; denote a &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;-dimensional basis function (e.g., Fourier basis) for encoding the time index, and &lt;script type=&quot;math/tex&quot;&gt;\Phi&lt;/script&gt; be the corresponding basis matrix, such that non-linear performance trends can be captured. Then an estimate of &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;’s future performance can be obtained as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
   \hat J_{k+1}(\pi) =  \phi(k+1)(\Phi^\top  \Phi)^{-1}\Phi^\top Y.
\end{align}&lt;/script&gt;

&lt;p&gt;Note that this approach naturally generalizes for the stationary setting as well, where the time series trend essentially corresponds to a horizontal line.&lt;/p&gt;

&lt;h2 id=&quot;optimizing-the-future-performance&quot;&gt;Optimizing the future performance&lt;/h2&gt;

&lt;p&gt;Using the time-series perspective, we saw how future performance of a given policy &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; can be estimated. Now, to search for a policy that would maximize the future performance, we take a differentiable programming perspective and treat &lt;em&gt;least-squares regression as a differentiable operator&lt;/em&gt;.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
     &lt;img src=&quot;/images/blog/optfuture/idea.png&quot; alt=&quot;idea&quot; style=&quot;vertical-align:middle; width: 40%; margin:20px 20px; border-radius:0%&quot; /&gt;
     &lt;/p&gt;

&lt;p&gt;The above image illustrates the main idea. The dotted lines represent that we cannot use conventional methods to compute or estimate gradients of &lt;script type=&quot;math/tex&quot;&gt;\hat J_{k+1}(\theta)&lt;/script&gt; as we have no samples from the future MDP. Instead, we estimate &lt;script type=&quot;math/tex&quot;&gt;J_{k+1}(\theta)&lt;/script&gt; as a composition of two programs: one which connects the policy’s parameters to its past performances, and the other which forecasts future performance as a function of these past performances. An optimization procedure can be  obtained by taking derivatives through this composition of programs to update policy parameters in a direction that maximizes future performance.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
    \frac{ d \hat J_{k+1}(\theta)}{d \theta} &amp;= \frac{d \Psi(\hat J_1(\theta), ..., \hat J_k(\theta))}{d \theta} \\
    %
    &amp;= \sum_{i=1}^k \underbrace{ \frac{\partial \Psi(\hat J_1(\theta), ..., \hat J_k(\theta))}{\partial \hat J_i(\theta)}}_{\text{(a)}}\cdot \underbrace{ \frac{d \hat J_i(\theta)}{d \theta}}_{(b)}. \,\,\, 
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;This decomposition has an elegant and intuitive interpretation. The terms assigned to &lt;script type=&quot;math/tex&quot;&gt;(a)&lt;/script&gt; correspond to how the future prediction would change as a function of past outcomes, and the terms in &lt;script type=&quot;math/tex&quot;&gt;(b)&lt;/script&gt; indicate how the past outcomes would change due to changes in the parameters of the policy &lt;script type=&quot;math/tex&quot;&gt;\pi_\theta&lt;/script&gt;. Here, term (b) can be obtained using standard off-policy gradient estimators and term (a) can be obtained as the following,&lt;/p&gt;

&lt;p&gt;\begin{align}
    \frac{\partial \hat J_{k+1}(\theta)}{\partial \hat J_i(\theta)}  &amp;amp;= \frac{\partial \phi(k+1)(\Phi^\top  \Phi)^{-1}\Phi^\top  Y}{\partial Y_i} &lt;br /&gt;
    %
    = [ \phi(k+1)(\Phi^\top  \Phi)^{-1}\Phi^\top  ]_i.
\end{align}&lt;/p&gt;

&lt;p&gt;Computationally, calculating &lt;script type=&quot;math/tex&quot;&gt;(\Phi^\top  \Phi)^{-1}&lt;/script&gt; is also cheap as &lt;script type=&quot;math/tex&quot;&gt;(\Phi^\top  \Phi)^{-1} \in \mathbb{R}^{d\times d}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; is the dimension of the basis and is extremely small compared to the number of data-points.&lt;/p&gt;

&lt;h2 id=&quot;an-intriguing-behavior&quot;&gt;An intriguing behavior&lt;/h2&gt;

&lt;p&gt;In the equation for the total derivative above, notice that as the scalar term (a) is multiplied by the off-policy gradient term (b), the gradient &lt;script type=&quot;math/tex&quot;&gt;d \hat J_{k+1}(\theta)/d \theta&lt;/script&gt; can be viewed as a weighted sum of off-policy policy gradients. To understand these weights better,  in the left of the following figure we provide a visualization for the weights obtained from term (a) for different episodes &lt;script type=&quot;math/tex&quot;&gt;i \in [1, 100]&lt;/script&gt;.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
     &lt;img align=&quot;left&quot; src=&quot;/images/blog/optfuture/weights.png&quot; alt=&quot;idea&quot; style=&quot;width: 30%; margin:10px 20px; border-radius:0%&quot; /&gt;
 &lt;img src=&quot;/images/blog/optfuture/Eval.png&quot; alt=&quot;Eval&quot; style=&quot;width: 35%;margin:20px 30px; border-radius:0%&quot; /&gt;
     &lt;/p&gt;

&lt;p&gt;Perhaps intriguingly, when an identity basis or Fourier basis is used for &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;, we notice  an occurrence of negative weights, suggesting that the optimization procedure should move towards a policy that had &lt;em&gt;lower&lt;/em&gt; performance in some of the past episodes!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;While this negative weighting seems unusual at first glance, it has an simple interpretation in hindsight.  Consider the image on the right above. 
Despite having lower estimates of return everywhere, &lt;script type=&quot;math/tex&quot;&gt;\pi_2&lt;/script&gt;’s rising trend suggests that it might have higher performance in the future, that is, &lt;script type=&quot;math/tex&quot;&gt;J_{k+1}(\pi_2) &gt; J_{k+1}(\pi_1)&lt;/script&gt;.
However, existing online learning methods like follow-the-leader maximize performance on all the past data uniformly (i.e., the green curve in the left image which gives equal weight to each episode). 
Similarly, the exponential weights (red curve in the left image) are representative of approaches that only optimize using data from recent episodes and discard previous data.
It can be checked that either of these methods that use only &lt;em&gt;non-negative&lt;/em&gt; weights cannot capture the trend to forecast &lt;script type=&quot;math/tex&quot;&gt;J_{k+1}(\pi_2) &gt; J_{k+1}(\pi_1)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt; 
In comparison, the weights obtained when using the identity basis (orange curve in the left image) would facilitate &lt;em&gt;minimization&lt;/em&gt; of performances in the distant past and maximization of performance in the recent past.
Intuitively, this means that it moves towards a policy whose performance is on a &lt;em&gt;linear rise&lt;/em&gt;, as it expects that policy to have better performance in the future. Fourier basis further extends this idea while removing the restriction on the trend to be linear. 
Observe the alternating sign of weights in the figure above when using the Fourier basis.
This suggests that the optimization procedure will take into account the &lt;em&gt;sequential differences&lt;/em&gt; in performances over the past, thereby favoring the policy that has shown the most performance increments in the past. Further, these weights are obtained automatically as a by-product of regression!&lt;/p&gt;

&lt;h3 id=&quot;going-further&quot;&gt;Going Further&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;One of the key drawbacks of using importance sampling is the variance resulting due to it. To mitigate variance, a popular method in stationary setting is to use weighted importance sampling (WIS). We show in our paper, that the idea of WIS can also be extended to non-stationary settings to significantly reduce variance. We encourage interested readers to check out more details and empirical results in the &lt;a href=&quot;https://arxiv.org/abs/2005.08158&quot;&gt;paper&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There have been several recent works in RL that provide better off-policy estimates, and at the same time, there is an extensive literature on analyzing non-stationary time-series data. While we took the preliminary steps in this framework to bring these two communities closer, we believe there is a lot more that can be done here.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In another &lt;a href=&quot;/blog/spin&quot;&gt;blogpost&lt;/a&gt;, I discuss our NeurIPS 2020 work that takes an initial step towards further bridging ideas from the time-series literature to provide safe policy-improvement in non-stationary MDPs.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yash Chandak</name></author><category term="timeseries" /><category term="nonstationary" /><category term="reinforcement" /><summary type="html">How do we create model-free algorithms that can search for a good policy in a non-stationary MDP?</summary></entry></feed>